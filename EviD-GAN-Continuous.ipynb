{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.logger import *  # Edit this file to match your local configuration\n",
    "from util.spectral_normilization import *\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import transforms, datasets\n",
    "DATA_FOLDER = 'dataset/CIFAR10'\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cifar_data():\n",
    "    compose = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "        \n",
    "    out_dir = '{}'.format(DATA_FOLDER)\n",
    "    return datasets.CIFAR10(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "batch_size=128\n",
    "data = cifar_data()\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_batches = len(data_loader)\n",
    "channels = 3\n",
    "leak = 0.1\n",
    "w_g = 4\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidLayer(nn.Module):\n",
    "    def __init__(self, n_features, n_output=1, d_hidden=128):\n",
    "        super(EvidLayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_output = n_output\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        self.l1 = SpectralNorm(nn.Linear(n_features, d_hidden))\n",
    "        self.l2 = SpectralNorm((nn.Linear(d_hidden, self.n_output)))\n",
    "        \n",
    "    def evidence(self, x):\n",
    "        return torch.nn.functional.softplus(x)\n",
    "    \n",
    "    def forward(self, inputs, y=None):\n",
    "        output = self.l1(inputs)\n",
    "        features = F.leaky_relu(output, 0.1, inplace=True)\n",
    "        d = self.l2(features)\n",
    "        if y is not None:\n",
    "            w_y = self.linear_y(y)\n",
    "            d = d + (features * w_y).sum(1, keepdim=True)\n",
    "        \n",
    "        mu, logv, logalpha, logbeta = torch.split(d, d.shape[-1]//self.n_output, dim=-1) \n",
    "        v = self.evidence(logv)+1e-12\n",
    "        alpha = self.evidence(logalpha) + 1\n",
    "        beta = self.evidence(logbeta)+1e-12\n",
    "        \n",
    "        return torch.cat([mu, v, alpha, beta],dim=-1)\n",
    "\n",
    "# Evidential discriminator network\n",
    "class EviDiscriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EviDiscriminator, self).__init__()\n",
    "        \n",
    "        self.wg=4 #8 for image size 64\n",
    "        \n",
    "        self.conv1 = SpectralNorm(nn.Conv2d(channels, 64, 3, stride=1, padding=(1,1)))\n",
    "        self.conv2 = SpectralNorm(nn.Conv2d(64, 64, 4, stride=2, padding=(1,1)))\n",
    "        self.conv3 = SpectralNorm(nn.Conv2d(64, 128, 3, stride=1, padding=(1,1)))\n",
    "        self.conv4 = SpectralNorm(nn.Conv2d(128, 128, 4, stride=2, padding=(1,1)))\n",
    "        self.conv5 = SpectralNorm(nn.Conv2d(128, 256, 3, stride=1, padding=(1,1)))\n",
    "        self.conv6 = SpectralNorm(nn.Conv2d(256, 256, 4, stride=2, padding=(1,1)))\n",
    "        self.linear = EvidLayer(self.wg *  self.wg * 256, n_output=4, d_hidden=128)\n",
    "\n",
    "    def evidence(self, x):\n",
    "        return torch.nn.functional.softplus(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        m = x\n",
    "        m = nn.LeakyReLU(leak)(self.conv1(m))\n",
    "        m = nn.LeakyReLU(leak)(self.conv2(m))\n",
    "        m = nn.LeakyReLU(leak)(self.conv3(m))\n",
    "        m = nn.LeakyReLU(leak)(self.conv4(m))\n",
    "        m = nn.LeakyReLU(leak)(self.conv5(m))\n",
    "        m = nn.LeakyReLU(leak)(self.conv6(m))\n",
    "        #print(m.shape)\n",
    "        feat = m.view(-1, self.wg * self.wg * 256)        \n",
    "        d = (self.linear(feat))\n",
    "        \n",
    "        return feat, d\n",
    "\n",
    "# Generator Network\n",
    "class GenerativeNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,z_dim):\n",
    "        super(GenerativeNet,self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        #self.dense = torch.nn.Linear(128,512 * 4 * 4)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, 4, stride=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=(1,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=(1,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=(1,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, channels, 3, stride=1, padding=(1,1)),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z.view(-1, self.z_dim, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    if torch.cuda.is_available(): return n.cuda()\n",
    "    return n\n",
    "\n",
    "# Initialized networks weights\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(0.00, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Network instances and init weights\n",
    "random.seed(1)\n",
    "generator = GenerativeNet(100)\n",
    "generator.apply(init_weights)\n",
    "evid_dist_r=[]\n",
    "evid_dist_f=[]\n",
    "\n",
    "discriminator = EviDiscriminator()\n",
    "  \n",
    "# Enable cuda if available\n",
    "if torch.cuda.is_available():\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "d_optimizer = Adam(discriminator.parameters(), lr=0.0002,betas=(0.5, 0.999))\n",
    "g_optimizer = Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = Variable(2*torch.ones(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_data_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(-2*torch.ones(size, 1))\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def NIG_NLL(y, gamma, v, alpha, beta, reduce=True):\n",
    "    twoBlambda = 2*beta*(1+v)\n",
    "    nll = 0.5*torch.log(np.pi/v)  \\\n",
    "        - alpha*torch.log(twoBlambda)  \\\n",
    "        + (alpha+0.5) * torch.log(v*(y-gamma)**2 + twoBlambda)  \\\n",
    "        + torch.lgamma(alpha)  \\\n",
    "        - torch.lgamma(alpha+0.5)\n",
    "    return torch.mean(nll)\n",
    "\n",
    "def NIG_Reg(y, gamma, v, alpha, beta, omega=0.01, reduce=True):\n",
    "    reg_loss=torch.nn.SmoothL1Loss()\n",
    "    error = reg_loss(gamma,y)\n",
    "    evi = 2*v+(alpha)\n",
    "    reg = error*evi\n",
    "    return torch.mean(reg)\n",
    "\n",
    "def loss_evid(evidential_output,y_true, coeff=1.0):\n",
    "    gamma, v, alpha, beta = torch.split(evidential_output, evidential_output.shape[-1]//4, dim=-1)    \n",
    "    loss_nll = NIG_NLL(y_true, gamma, v, alpha, beta)\n",
    "    loss_reg = NIG_Reg(y_true, gamma, v, alpha, beta)\n",
    "    return loss_nll + coeff * loss_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_Sup for AsymReg\n",
    "def loss_sup(out1, others, temperature=0.1):\n",
    "   \n",
    "    N = out1.size(0)\n",
    "    out1 = F.normalize(out1)\n",
    "    others = F.normalize(others)\n",
    "    _out = [out1, others]\n",
    "    outputs = torch.cat(_out, dim=0)\n",
    "    sim_matrix = outputs @ outputs.t()\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "    sim_matrix.fill_diagonal_(-5e4)\n",
    "\n",
    "    mask = torch.zeros_like(sim_matrix)\n",
    "    mask[N:,N:] = 1\n",
    "    mask.fill_diagonal_(0)\n",
    "\n",
    "    sim_matrix = sim_matrix[N:]\n",
    "    mask = mask[N:]\n",
    "    mask = mask / mask.sum(1, keepdim=True)\n",
    "\n",
    "    lsm = F.log_softmax(sim_matrix, dim=1)\n",
    "    lsm = lsm * mask\n",
    "    d_loss = -lsm.sum(1).mean()\n",
    "    return d_loss\n",
    "\n",
    "# L_G_reg for AsymReg\n",
    "def loss_nt_xent(out1, out2, temperature=0.1,  normalize=True):\n",
    "   \n",
    "    assert out1.size(0) == out2.size(0)\n",
    "    if normalize:\n",
    "        out1 = F.normalize(out1)\n",
    "        out2 = F.normalize(out2)\n",
    "    N = out1.size(0)\n",
    "\n",
    "    _out = [out1, out2]\n",
    "    outputs = torch.cat(_out, dim=0)\n",
    "\n",
    "    sim_matrix = outputs @ outputs.t()\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    sim_matrix.fill_diagonal_(-5e4)\n",
    "    sim_matrix = F.log_softmax(sim_matrix, dim=1)\n",
    "    loss = -torch.sum(sim_matrix[:N, N:].diag() + sim_matrix[N:, :N].diag()) / (2*N)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer,real_data,fake_data):\n",
    "    # Reset gradients  \n",
    "    optimizer.zero_grad()\n",
    "    N=real_data.size(0)\n",
    "    # 1.1 Train on Real Data\n",
    "    feat, prediction = discriminator(torch.cat([real_data,fake_data],dim=0))    \n",
    "  \n",
    "    prediction_real, prediction_fake= prediction[:N],prediction[N:]\n",
    "\n",
    "    # Calculate error and backpropagate\n",
    "    error_dis = (loss_evid(prediction_real,real_data_target(real_data.size(0)))+loss_evid(prediction_fake,fake_data_target(real_data.size(0))))\n",
    "    (error_dis).backward()\n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    return error_dis, prediction_real.mean().item(), prediction_fake.mean().item()\n",
    "\n",
    "def train_generator(optimizer,fake_data):\n",
    "    # 2. Train Generator\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    feat, prediction_fake = discriminator(fake_data)\n",
    "    d_pred_fake2 = prediction_fake.mean().item()\n",
    "    error_gen = loss_evid(prediction_fake,real_data_target(fake_data.size(0)))\n",
    "    (error_gen).backward()\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error_gen,d_pred_fake2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)\n",
    "n_critic=1\n",
    "alpha=1.\n",
    "use_cuda=True\n",
    "g_step=0\n",
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logger = Logger(model_name='EviD-GAN-C', data_name='CIFAR10')\n",
    "g_error,d_pred_fake2,fake_data=0,0,0\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch%2==0: \n",
    "        display.clear_output(True)\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "       \n",
    "        step = epoch * len(data_loader) + n_batch + 1\n",
    "            \n",
    "        # 1. Train Discriminator\n",
    "        real_data = Variable(real_batch)\n",
    "        #a_real_data = Variable(a_real_batch)\n",
    "        if torch.cuda.is_available(): \n",
    "            real_data = real_data.cuda()\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(real_data.size(0))).detach()\n",
    "    \n",
    "        # Train D\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, \n",
    "                                                                real_data,fake_data)\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)   \n",
    "        \n",
    "        # 2. Train Generator\n",
    "        if step % n_critic == 0:\n",
    "            # Generate fake data\n",
    "            fake_data = generator(noise(real_batch.size(0)))\n",
    "            # Train G\n",
    "            g_error,d_pred_fake2 = train_generator(g_optimizer,fake_data)\n",
    "            g_step=g_step+1\n",
    "        # Log error\n",
    "        \n",
    "        # Display Progress\n",
    "        if (n_batch) % 500 == 0:\n",
    "            #display.clear_output(True)\n",
    "            # Display Images\n",
    "            test_images = generator(test_noise).data.cpu()\n",
    "            #test_images = generator(noise_simclr(real_data[:16])).data.cpu()\n",
    "            logger.log_images(test_images, num_test_samples, step, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake,d_pred_fake2\n",
    "            )\n",
    "        # Model Checkpoints\n",
    "        if g_step >= 1e4 and g_step % 2e3==0:\n",
    "            logger.save_models(generator, discriminator, g_step)\n",
    "        if(g_step >= 200000):\n",
    "            break\n",
    "    if(g_step >= 200000):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
